---
title: "MovieLens document"
author: "Robert van der Heijden"
date: "2/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

# Content

1. Summary section: describes the dataset and summarizes the goal of the project and key steps that were performed
2. Analysis section: explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach
3. Results section: presents the modeling results and discusses the model performance
4. Conclusion section: gives a brief summary of the report, its limitations and future work



# 1) Summary section
For this project, we will be creating a movie recommendation system using the MovieLens dataset. The full version of movielens includes millions of ratings. We will use the 10M version of the MovieLens dataset to make the computation a little easier.

The goal of this project is to train a machine learning algorithm using the inputs in the 10M version of the MovieLens dataset. The predictions from this machine learning algorithm will be compared to the true ratings in the validation set using RMSE. 



# 2) Analysis Section
The first step is to create an EDX set and validation set.

  ## Create edx set, validation set

```{r edx, echo = FALSE}

# Note: this process could take a couple of minutes
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
# set.seed(1, sample.kind="Rounding")
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

## Data Exploration (Before Data Cleaning)

### What is in the Edx data set?

1. The first ten lines of the dataset:

```{r head, echo=FALSE}
library(knitr)
kable(edx[1:10,])
```

2. The summary of the statistics of the dataset:

```{r summary, echo=FALSE}
kable(summary(edx))
```

```{r movies, echo=FALSE}
library(tidyverse)
Titles <- edx %>% select(movieId) %>% distinct()
```
```{r users, echo=FALSE}
Users <- edx %>% select(userId) %>% distinct()
```
|  |  |
| --- | --- |
| 3. The number of rows is: | `r nrow(edx)` |
| 4. The number of columns is: | `r ncol(edx)` |
| 5. How many different movies are in the edx dataset?|`r nrow(Titles)`|
| 6. How many different users are in the edx dataset?|`r nrow(Users)`|

7. Which movie has the greatest number of ratings?

```{r number_ratings, echo=FALSE}
library(tidyverse)
number_ratings <- edx %>% group_by(title) %>% summarise(number = n()) %>% arrange(desc(number))
kable(number_ratings[1:10,])
```

## Data Cleaning
  
  - Modify the year as a column in the edx & validation datasets
  
  ## Year added to edx set, validation set
```{r Modify_year, echo=FALSE}
edx <- edx %>% mutate(year = as.numeric(str_sub(edx$title,-5,-2)))
validation <- validation %>% mutate(year = as.numeric(str_sub(validation$title,-5,-2)))
```

## Data Exploration (After Data Cleaning)
  
### How are the ratings distributed (histogram)? 
      
```{r histogram_ratings, echo=FALSE}
edx %>% qplot(rating, geom ="histogram", bins = 10, data = ., color = I("black"))
```

### How are the ratings distributed over the years (histogram)? 

```{r scatterplot_year, echo=FALSE}
edx %>% qplot(year, geom ="histogram", bins = 10, data = ., color = I("black"))
```

## Modelling Approach
 
- RMSE will be used to evaluate how close the predictions are to the true values in the validation set.
 
```{r RMSE, echo=FALSE}
RMSE <- function(true_ratings, predicted_ratings){sqrt(mean((true_ratings - predicted_ratings)^2))}
```
 
### Building the recommendation system
```{r mu_hat, echo=FALSE}
mu_hat <- mean(edx$rating)
```

1. We start by building the simplest possible recommendation system. We're going to predict the same rating for all movies. The average that we predict is: 

`r mu_hat`

```{r naive_rmse, echo=FALSE}
naive_rmse <- RMSE(mu_hat, validation$rating)
```

How well does this model? The deviation on average is:

`r naive_rmse`

Now because as we go along we will be comparing different approaches, we're going to create a table that's going to store the results that we obtain as we go along.
     
```{r table_average, echo=FALSE}
rmse_results <- data.frame(Method = "Just the average", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

2. In the second step, we are going to take the movie effect into account. 

```{r movie_effect, echo=FALSE}
mu <- mean(edx$rating)
movie_avgs <- edx %>% group_by(movieId) %>% summarize(b_i = mean(rating-mu))
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))
```

The histogram shows the deviation of a movie rating from the average rating. You can see that these estimates vary substantially. Some movies are good, other movies are bad.
 
*Updated RMSE table:*
```{r table_movie_effect, echo = FALSE}
predicted_ratings <- mu + validation %>% left_join(movie_avgs, by='movieId') %>% .$b_i
model_1_rmse <- RMSE(validation$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, data.frame(Method="Movie Effect Model", RMSE = model_1_rmse))
rmse_results %>% knitr::kable()

```

3. In the third step, we are going to take the movie effect and user effect into account.

```{r user_effect, echo = FALSE}
edx %>% group_by(userId) %>% summarize(b_u = mean(rating)) %>% filter(n()>=100) %>% ggplot(aes(b_u)) + geom_histogram(bins = 30, color = "black")
```

The histogram shows the variability of a movie rating of a user. There is substantial variability across users
  
*Updated RMSE table:*
  
```{r table_user_effect, echo = FALSE}

user_avgs <- edx %>% left_join(movie_avgs, by='movieId') %>% group_by(userId) %>% summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- validation %>% left_join(movie_avgs, by='movieId') %>% left_join(user_avgs, by='userId') %>% mutate(pred = mu + b_i + b_u) %>% .$pred
model_2_rmse <- RMSE(validation$rating, predicted_ratings)
rmse_results <- bind_rows(rmse_results, data.frame(Method="Movie + User Effects Model", RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()

```

4. in the fourth step, we are going to regularize the movie and user effect.

Best and worst movies are rated by a few users, but have a big impact on our model. We use regularization to improve our model
    
Top 5 Best predicted movies:
```{r best_movies, echo = FALSE}
movie_titles <- edx %>% select(movieId, title) %>% distinct()
# %>% left_join(movie_titles, by="movieId") %>% arrange(desc(b_i)) %>% select(title, b_i)

edx %>% count(movieId) %>% left_join(movie_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(desc(b_i)) %>% select(title, b_i,n) %>% slice(1:5) %>% knitr::kable()
```

Top 5 Worst predicted movies:
```{r worst_movies, echo = FALSE}
edx %>% count(movieId) %>% left_join(movie_avgs) %>% left_join(movie_titles, by="movieId") %>% arrange(b_i) %>% select(title, b_i,n) %>% slice(1:5) %>% knitr::kable()
```    

Large errors can increase our residual mean squared error, so we would rather be conservative when we're not sure. Regularization permits us to penalize large estimates that come from small sample sizes.
  
First we have to pick the optimal tuning parameter lambda. 
  
```{r lambda, echo = FALSE}

lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(edx$rating)

  b_i <- edx %>% group_by(movieId) %>% summarize(b_i = sum(rating-mu)/(n()+l))

  b_u <- edx %>% left_join(b_i, by="movieId") %>% group_by(userId) %>% summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- validation %>% left_join(b_i, by = "movieId") %>% left_join(b_u, by = "userId") %>% mutate(pred = mu + b_i + b_u) %>% pull(pred)
  
  return(RMSE(predicted_ratings, validation$rating))})

qplot(lambdas, rmses)

```
  
The optimal lambda is:
  
```{r optimal_lambda, echo=FALSE}
lambda <- lambdas[which.min(rmses)]
lambda
```
  
  
*Updated RMSE table:*
```{r regularized, echo=FALSE}
rmse_results <- bind_rows(rmse_results, data.frame(Method="Regularized Movie + User Effects Model", RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

# 3) Results Section

RMSE overview

The RMSE values for the used models are shown below:

```{r conclusion, echo = FALSE}
rmse_results %>% knitr::kable()
```

The RMSE table shows an improvement of the model over the different assumptions. The simplest model ‘Using mean only’ calculates a RMSE of more than 1.06, which means, on average, we miss the rating by one star. Incorporating ‘Movie effect’ and ‘Movie and User effect’ in our model gives an improvement of 11% and 18.4% respectively. 
    
A deeper insight into the data revealed some data points have large effect on errors. So a regularization model was used to penalize these kind of data points. The final RMSE is 0.864986 with an improvement over 18.45% with respect to the baseline model. 
    
Other sources of variation (e.g. the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well) can be added to the model to further improve the predictability of the model.  
    
# 4) Conclusion Section

For this project, we created a movie recommendation system using the MovieLens dataset. The full version of movielens includes millions of ratings. We used the 10M version of the MovieLens dataset to make the computation a little easier.
    
The goal of this project is to train a machine learning algorithm using the inputs in the 10M version of the MovieLens dataset. The predictions from this machine learning algorithm were compared to the true ratings in the validation set using RMSE. 

We started with a simple model, using the 'mean rating' only, and added 'Movie Effects' and 'User Effects'. Our final model included regularization, that improved the final RMSE to 0.864986. This was an improvement over 18.45% with respect to the baseline model.

Other sources of variation (e.g. the fact that groups of movies have similar rating patterns and groups of users have similar rating patterns as well) can be added to the model to further improve the predictability of the model.